{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPr9l0sJ7+8VjixwKNJ/QDL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BehnoodRasti/Unmixing-Tutorial/blob/main/SUnCNNCollab_DC1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGZHzt_qoJf6",
        "outputId": "1889e97a-fa0d-460a-c716-417eaed2c0ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SUnCNN'...\n",
            "remote: Enumerating objects: 97, done.\u001b[K\n",
            "remote: Counting objects: 100% (97/97), done.\u001b[K\n",
            "remote: Compressing objects: 100% (57/57), done.\u001b[K\n",
            "remote: Total 97 (delta 42), reused 92 (delta 40), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (97/97), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/BehnoodRasti/SUnCNN.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import matplotlib.pyplot as plt\n",
        "#%matplotlib inline\n",
        "\n",
        "import os\n",
        "#os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
        "\n",
        "import numpy as np\n",
        "from SUnCNN.models import *\n",
        "\n",
        "import torch\n",
        "import torch.optim\n",
        "\n",
        "from skimage.metrics  import peak_signal_noise_ratio as compare_psnr\n",
        "from skimage.metrics  import mean_squared_error as compare_mse\n",
        "\n",
        "from SUnCNN.utils.denoising_utils import *\n",
        "\n",
        "from skimage._shared import *\n",
        "from skimage.util import *\n",
        "from skimage.metrics.simple_metrics import _as_floats\n",
        "from skimage.metrics.simple_metrics import mean_squared_error\n",
        "\n",
        "\n",
        "from SUnCNN.UtilityMine import *\n",
        "from SUnCNN.utils.sr_utils import tv_loss\n",
        "from numpy import linalg as LA\n",
        "\n",
        "torch.backends.cudnn.enabled = True\n",
        "torch.backends.cudnn.benchmark =True\n",
        "dtype = torch.cuda.FloatTensor\n",
        "\n",
        "PLOT = False\n",
        "import scipy.io\n",
        "#%%\n",
        "fname2  = \"SUnCNN/Data/DC1/Y_clean.mat\"\n",
        "mat2 = scipy.io.loadmat(fname2)\n",
        "img_np_gt = mat2[\"Y_clean\"]\n",
        "img_np_gt = img_np_gt.transpose(2,0,1)\n",
        "[p1, nr1, nc1] = img_np_gt.shape\n",
        "#%%\n",
        "fname3  = \"SUnCNN/Data/DC1/XT.mat\"\n",
        "mat3 = scipy.io.loadmat(fname3)\n",
        "A_true_np = mat3[\"XT\"]\n",
        "\n",
        "#%%\n",
        "fname4  = \"SUnCNN/Data/DC1/EE.mat\"\n",
        "mat4 = scipy.io.loadmat(fname4)\n",
        "EE = mat4[\"EE\"]\n",
        "#%%\n",
        "LibS=EE.shape[1]\n",
        "#%%\n",
        "npar=np.zeros((1,3))\n",
        "npar[0,0]=13.3\n",
        "npar[0,1]=41.4\n",
        "npar[0,2]=130.8\n",
        "#npar[0,3]=367\n",
        "tol1=npar.shape[1]\n",
        "tol2=1\n",
        "save_result=False\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "rmax=5"
      ],
      "metadata": {
        "id": "oNp8Wu7DodSa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for fi in tqdm(range(1)):\n",
        "    for fj in tqdm(range(tol2)):\n",
        "            #%%\n",
        "        img_np_gt=np.clip(img_np_gt, 0, 1)\n",
        "        img_noisy_np = add_noise(img_np_gt, 1/npar[0,fi])#11.55 20 dB, 36.7 30 dB, 116.5 40 dB\n",
        "        print(compare_snr(img_np_gt, img_noisy_np))\n",
        "        img_resh=np.reshape(img_noisy_np,(p1,nr1*nc1))\n",
        "        V, SS, U = scipy.linalg.svd(img_resh, full_matrices=False)\n",
        "        PC=np.diag(SS)@U\n",
        "        img_resh_DN=V[:,:rmax]@PC[:rmax,:]\n",
        "        img_noisy_np=np.reshape(np.clip(img_resh_DN, 0, 1),(p1,nr1,nc1))\n",
        "        INPUT = 'noise' # 'meshgrid'\n",
        "        pad = 'reflection'\n",
        "        need_bias=True\n",
        "        OPT_OVER = 'net' # 'net,input'\n",
        "        \n",
        "        # \n",
        "        reg_noise_std = 0.0\n",
        "        LR1 = 0.001\n",
        "        \n",
        "        OPTIMIZER1='adam'# 'RMSprop'#'adam' # 'LBFGS'\n",
        "        show_every = 500\n",
        "        exp_weight=0.99\n",
        "        if fi==0:\n",
        "            num_iter1 = 40\n",
        "        elif fi==1:\n",
        "            num_iter1 = 8000\n",
        "        elif fi==2:\n",
        "            num_iter1 = 12000\n",
        "        input_depth =img_noisy_np.shape[0]\n",
        "        class CAE_AbEst(nn.Module):\n",
        "            def __init__(self):\n",
        "                super(CAE_AbEst, self).__init__()\n",
        "                self.conv1 = nn.Sequential(\n",
        "                    UnmixArch(\n",
        "                            input_depth, EE.shape[1],\n",
        "                            # num_channels_down = [8, 16, 32, 64, 128], \n",
        "                            # num_channels_up   = [8, 16, 32, 64, 128],\n",
        "                            # num_channels_skip = [4, 4, 4, 4, 4], \n",
        "                            num_channels_down = [ 256],\n",
        "                            num_channels_up =   [ 256],\n",
        "                            num_channels_skip =    [ 4],  \n",
        "                            filter_size_up = 3,filter_size_down = 3,  filter_skip_size=1,\n",
        "                            upsample_mode='bilinear', # downsample_mode='avg',\n",
        "                            need1x1_up=True,\n",
        "                            need_sigmoid=True, need_bias=True, pad=pad, act_fun='LeakyReLU').type(dtype)\n",
        "                )\n",
        "        \n",
        "            def forward(self, x):\n",
        "                x = self.conv1(x)\n",
        "                return x\n",
        "\n",
        "        net1 = CAE_AbEst()\n",
        "        net1.cuda()\n",
        "        print(net1)\n",
        "\n",
        "        # Compute number of parameters\n",
        "        s  = sum([np.prod(list(p11.size())) for p11 in net1.parameters()]); \n",
        "        print ('Number of params: %d' % s)\n",
        "        \n",
        "        # Loss\n",
        "        mse = torch.nn.MSELoss().type(dtype)\n",
        "        img_noisy_torch = np_to_torch(img_noisy_np).type(dtype)\n",
        "        # if fk==0:\n",
        "        net_input1 = get_noise(input_depth, INPUT,\n",
        "          (img_noisy_np.shape[1], img_noisy_np.shape[2])).type(dtype).detach()\n",
        "        net_input1 = img_noisy_torch \n",
        "        E_torch = np_to_torch(EE).type(dtype)\n",
        "        #%%\n",
        "        net_input_saved = net_input1.detach().clone()\n",
        "        noise = net_input1.detach().clone()\n",
        "        out_avg = None\n",
        "        out_HR_avg= None\n",
        "        last_net = None\n",
        "        RMSE_LR_last = 0\n",
        "        loss=np.zeros((num_iter1,1))\n",
        "        AE=np.zeros((num_iter1,1))\n",
        "        i = 0\n",
        "        def closure1():\n",
        "            \n",
        "            global i, RMSE_LR, RMSE_LR_ave, RMSE_HR, out_LR_np, out_avg_np, out_LR\\\n",
        "                , out_avg,out_HR_np, out_HR_avg, out_HR_avg_np, RMSE_LR_last, last_net\\\n",
        "                    , net_input,RMSE_LR_avg,RMSE_HR_avg,RE_HR_avg, RE_HR, Eest,loss,AE\\\n",
        "                       , MAE_LR,MAE_LR_avg,MAE_HR,MAE_HR_avg\n",
        "            \n",
        "            if reg_noise_std > 0:\n",
        "                net_input = net_input_saved + (noise.normal_() * reg_noise_std)\n",
        "            \n",
        "            out_LR = net1(net_input1)\n",
        "            out_HR=torch.mm(E_torch.view(p1,LibS),out_LR.view(LibS,nr1*nc1))\n",
        "            # Smoothing\n",
        "            if out_avg is None:\n",
        "                out_avg = out_LR.detach()\n",
        "                out_HR_avg = out_HR.detach()\n",
        "            else:\n",
        "                out_avg = out_avg * exp_weight + out_LR.detach() * (1 - exp_weight)\n",
        "                out_HR_avg = out_HR_avg * exp_weight + out_HR.detach() * (1 - exp_weight)\n",
        "\n",
        "        #%%\n",
        "            out_HR=out_HR.view((1,p1,nr1,nc1))\n",
        "            total_loss = mse(img_noisy_torch, out_HR)\n",
        "            total_loss.backward()\n",
        "            if True:\n",
        "             out_LR_np = out_LR.detach().cpu().squeeze().numpy()\n",
        "             out_avg_np = out_avg.detach().cpu().squeeze().numpy()\n",
        "             SRE=10*np.log10(LA.norm(A_true_np.astype(np.float32).reshape((EE.shape[1],nr1*nc1)),'fro')/LA.norm((A_true_np.astype(np.float32)- np.clip(out_LR_np, 0, 1)).reshape((EE.shape[1],nr1*nc1)),'fro'))\n",
        "             SRE_avg=10*np.log10(LA.norm(A_true_np.astype(np.float32).reshape((EE.shape[1],nr1*nc1)),'fro')/LA.norm((A_true_np.astype(np.float32)- np.clip(out_avg_np, 0, 1)).reshape((EE.shape[1],nr1*nc1)),'fro'))\n",
        "             MAE_LR= 100*np.mean(abs(A_true_np.astype(np.float32)- np.clip(out_LR_np, 0, 1)))\n",
        "             MAE_LR_avg= 100*np.mean(abs(A_true_np.astype(np.float32)- np.clip(out_avg_np, 0, 1)))\n",
        "             print ('Iteration %05d    Loss %f   MAE_LR: %f MAE_LR_avg: %f  SRE: %f SRE_avg: %f' % (i, total_loss.item(), MAE_LR, MAE_LR_avg, SRE, SRE_avg))\n",
        "\n",
        "            if  PLOT and i % show_every == 0:\n",
        "                out_LR_np = torch_to_np(out_LR)\n",
        "                out_avg_np = torch_to_np(out_avg)\n",
        "        #        plot_image_grid([np.clip(out_np, 0, 1), \n",
        "        #                         np.clip(torch_to_np(out_avg), 0, 1)], factor=figsize, nrow=1)\n",
        "                \n",
        "                # out_LR_np = np.clip(out_LR_np, 0, 1)\n",
        "                # out_avg_np = np.clip(out_avg_np, 0, 1)\n",
        "                \n",
        "                # f, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True, figsize=(10,10))\n",
        "                # ax1.imshow(np.stack((out_LR_np[2,:,:],out_LR_np[1,:,:],out_LR_np[0,:,:]),2))\n",
        "                # ax2.imshow(np.stack((out_avg_np[2,:,:],out_avg_np[1,:,:],out_avg_np[0,:,:]),2))\n",
        "                # ax3.imshow(np.stack((A_true_np[2,:,:],A_true_np[1,:,:],A_true_np[0,:,:]),2))\n",
        "                # plt.show()\n",
        "                plt.plot(out_LR_np.reshape(LibS,nr1*nc1))\n",
        "            loss[i]=total_loss.item() \n",
        "            i += 1\n",
        "        \n",
        "            return total_loss\n",
        "        \n",
        "        p11 = get_params(OPT_OVER, net1, net_input1)\n",
        "        optimize(OPTIMIZER1, p11, closure1, LR1, num_iter1)\n",
        "        if 0:\n",
        "            out_LR_np = out_LR.detach().cpu().squeeze().numpy()\n",
        "            out_avg_np = out_avg.detach().cpu().squeeze().numpy()\n",
        "            MAE_LR_avg= 100*np.mean(abs(A_true_np.astype(np.float32)- np.clip(out_avg_np, 0, 1)))\n",
        "            MAE_LR= 100*np.mean(abs(A_true_np.astype(np.float32)- np.clip(out_LR_np, 0, 1)))\n",
        "            SRE=10*np.log10(LA.norm(A_true_np.astype(np.float32).reshape((EE.shape[1],nr1*nc1)),'fro')/LA.norm((A_true_np.astype(np.float32)- np.clip(out_LR_np, 0, 1)).reshape((EE.shape[1],nr1*nc1)),'fro'))\n",
        "            SRE_avg=10*np.log10(LA.norm(A_true_np.astype(np.float32).reshape((EE.shape[1],nr1*nc1)),'fro')/LA.norm((A_true_np.astype(np.float32)- np.clip(out_avg_np, 0, 1)).reshape((EE.shape[1],nr1*nc1)),'fro'))\n",
        "            print ('Iteration %05d  MAE_LR: %f MAE_LR_avg: %f  SRE: %f SRE_avg: %f ' % (i, MAE_LR, MAE_LR_avg, SRE, SRE_avg))\n",
        "        # if  save_result is True:\n",
        "        #      scipy.io.savemat(\"C:/Users/behnood/Desktop/Sparse Unmixing/Results/Sim2/demo1/10runs/out_avg_np%01d%01d.mat\" % (fi+2,fj+1),\n",
        "        #                     {'out_avg_np%01d%01d' % (fi+2, fj+1):out_avg_np.transpose(1,2,0)})\n",
        "        #      scipy.io.savemat(\"C:/Users/behnood/Desktop/Sparse Unmixing/Results/Sim2/demo1/10runs/out_LR_np%01d%01d.mat\" % (fi+2,fj+1),\n",
        "        #                     {'out_LR_np%01d%01d' % (fi+2, fj+1):out_LR_np.transpose(1,2,0)})\n",
        "#%%\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSpuKXaqrnto",
        "outputId": "18c547b2-0d74-45d1-decf-197cfb30f1e7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/1 [00:00<?, ?it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20.184469252756486\n",
            "CAE_AbEst(\n",
            "  (conv1): Sequential(\n",
            "    (0): Sequential(\n",
            "      (1): Concat(\n",
            "        (0): Sequential(\n",
            "          (1): Sequential(\n",
            "            (0): ReflectionPad2d((0, 0, 0, 0))\n",
            "            (1): Conv2d(224, 4, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (3): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (1): Sequential(\n",
            "            (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "            (1): Conv2d(224, 256, kernel_size=(3, 3), stride=(2, 2))\n",
            "          )\n",
            "          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (3): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "          (4): Sequential(\n",
            "            (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "            (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "          )\n",
            "          (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (6): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "          (7): Upsample(scale_factor=2.0, mode=bilinear)\n",
            "        )\n",
            "      )\n",
            "      (2): BatchNorm2d(260, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (3): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(260, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "      )\n",
            "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (6): Sequential(\n",
            "        (0): ReflectionPad2d((0, 0, 0, 0))\n",
            "        (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (8): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (9): Sequential(\n",
            "        (0): ReflectionPad2d((0, 0, 0, 0))\n",
            "        (1): Conv2d(256, 240, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (10): Softmax(dim=None)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Number of params: 1836676\n",
            "Starting optimization with ADAM\n",
            "Iteration 00000    Loss 0.107289   MAE_LR: 0.817500 MAE_LR_avg: 0.817500  SRE: 0.023297 SRE_avg: 0.023297\n",
            "Iteration 00001    Loss 0.097443   MAE_LR: 0.816608 MAE_LR_avg: 0.817491  SRE: -0.042777 SRE_avg: 0.023317\n",
            "Iteration 00002    Loss 0.087888   MAE_LR: 0.815662 MAE_LR_avg: 0.817473  SRE: -0.157877 SRE_avg: 0.023325\n",
            "Iteration 00003    Loss 0.078136   MAE_LR: 0.813878 MAE_LR_avg: 0.817437  SRE: -0.195686 SRE_avg: 0.023401\n",
            "Iteration 00004    Loss 0.068475   MAE_LR: 0.810858 MAE_LR_avg: 0.817371  SRE: -0.177597 SRE_avg: 0.023671\n",
            "Iteration 00005    Loss 0.060059   MAE_LR: 0.808099 MAE_LR_avg: 0.817276  SRE: -0.241284 SRE_avg: 0.024044\n",
            "Iteration 00006    Loss 0.051626   MAE_LR: 0.804900 MAE_LR_avg: 0.817143  SRE: -0.201209 SRE_avg: 0.024662\n",
            "Iteration 00007    Loss 0.043944   MAE_LR: 0.802627 MAE_LR_avg: 0.816987  SRE: -0.242182 SRE_avg: 0.025306\n",
            "Iteration 00008    Loss 0.037709   MAE_LR: 0.800181 MAE_LR_avg: 0.816804  SRE: -0.298957 SRE_avg: 0.026000\n",
            "Iteration 00009    Loss 0.032254   MAE_LR: 0.797046 MAE_LR_avg: 0.816583  SRE: -0.330041 SRE_avg: 0.026836\n",
            "Iteration 00010    Loss 0.027901   MAE_LR: 0.793435 MAE_LR_avg: 0.816319  SRE: -0.358136 SRE_avg: 0.027869\n",
            "Iteration 00011    Loss 0.024143   MAE_LR: 0.789757 MAE_LR_avg: 0.816014  SRE: -0.412882 SRE_avg: 0.029026\n",
            "Iteration 00012    Loss 0.020906   MAE_LR: 0.785949 MAE_LR_avg: 0.815667  SRE: -0.470690 SRE_avg: 0.030260\n",
            "Iteration 00013    Loss 0.018259   MAE_LR: 0.781871 MAE_LR_avg: 0.815273  SRE: -0.499931 SRE_avg: 0.031633\n",
            "Iteration 00014    Loss 0.016019   MAE_LR: 0.777639 MAE_LR_avg: 0.814826  SRE: -0.513109 SRE_avg: 0.033188\n",
            "Iteration 00015    Loss 0.014107   MAE_LR: 0.773266 MAE_LR_avg: 0.814325  SRE: -0.524941 SRE_avg: 0.034940\n",
            "Iteration 00016    Loss 0.012496   MAE_LR: 0.768785 MAE_LR_avg: 0.813770  SRE: -0.536238 SRE_avg: 0.036885\n",
            "Iteration 00017    Loss 0.011145   MAE_LR: 0.764092 MAE_LR_avg: 0.813159  SRE: -0.543393 SRE_avg: 0.039036\n",
            "Iteration 00018    Loss 0.009978   MAE_LR: 0.759035 MAE_LR_avg: 0.812485  SRE: -0.544047 SRE_avg: 0.041409\n",
            "Iteration 00019    Loss 0.008966   MAE_LR: 0.753562 MAE_LR_avg: 0.811743  SRE: -0.539593 SRE_avg: 0.044010\n",
            "Iteration 00020    Loss 0.008098   MAE_LR: 0.747629 MAE_LR_avg: 0.810934  SRE: -0.533526 SRE_avg: 0.046836\n",
            "Iteration 00021    Loss 0.007343   MAE_LR: 0.741079 MAE_LR_avg: 0.810052  SRE: -0.526908 SRE_avg: 0.049879\n",
            "Iteration 00022    Loss 0.006676   MAE_LR: 0.733793 MAE_LR_avg: 0.809091  SRE: -0.516709 SRE_avg: 0.053170\n",
            "Iteration 00023    Loss 0.006086   MAE_LR: 0.725338 MAE_LR_avg: 0.808040  SRE: -0.499041 SRE_avg: 0.056758\n",
            "Iteration 00024    Loss 0.005564   MAE_LR: 0.715902 MAE_LR_avg: 0.806885  SRE: -0.471452 SRE_avg: 0.060714\n",
            "Iteration 00025    Loss 0.005111   MAE_LR: 0.707101 MAE_LR_avg: 0.805611  SRE: -0.434549 SRE_avg: 0.065108\n",
            "Iteration 00026    Loss 0.004730   MAE_LR: 0.700463 MAE_LR_avg: 0.804212  SRE: -0.391939 SRE_avg: 0.069962\n",
            "Iteration 00027    Loss 0.004415   MAE_LR: 0.695316 MAE_LR_avg: 0.802695  SRE: -0.347195 SRE_avg: 0.075264\n",
            "Iteration 00028    Loss 0.004146   MAE_LR: 0.690974 MAE_LR_avg: 0.801073  SRE: -0.303198 SRE_avg: 0.080948\n",
            "Iteration 00029    Loss 0.003895   MAE_LR: 0.687126 MAE_LR_avg: 0.799379  SRE: -0.260178 SRE_avg: 0.086925\n",
            "Iteration 00030    Loss 0.003639   MAE_LR: 0.683511 MAE_LR_avg: 0.797645  SRE: -0.214004 SRE_avg: 0.093075\n",
            "Iteration 00031    Loss 0.003374   MAE_LR: 0.679988 MAE_LR_avg: 0.795903  SRE: -0.160971 SRE_avg: 0.099316\n",
            "Iteration 00032    Loss 0.003122   MAE_LR: 0.676490 MAE_LR_avg: 0.794176  SRE: -0.100671 SRE_avg: 0.105606\n",
            "Iteration 00033    Loss 0.002904   MAE_LR: 0.673042 MAE_LR_avg: 0.792476  SRE: -0.035466 SRE_avg: 0.111940\n",
            "Iteration 00034    Loss 0.002720   MAE_LR: 0.669697 MAE_LR_avg: 0.790801  SRE: 0.030896 SRE_avg: 0.118342\n",
            "Iteration 00035    Loss 0.002555   MAE_LR: 0.666450 MAE_LR_avg: 0.789141  SRE: 0.094301 SRE_avg: 0.124843\n",
            "Iteration 00036    Loss 0.002393   MAE_LR: 0.663328 MAE_LR_avg: 0.787484  SRE: 0.150839 SRE_avg: 0.131464\n",
            "Iteration 00037    Loss 0.002229   MAE_LR: 0.660336 MAE_LR_avg: 0.785814  SRE: 0.197645 SRE_avg: 0.138217\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1/1 [00:02<00:00,  2.77s/it]\n",
            "100%|██████████| 1/1 [00:02<00:00,  2.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 00038    Loss 0.002075   MAE_LR: 0.657579 MAE_LR_avg: 0.784120  SRE: 0.232961 SRE_avg: 0.145112\n",
            "Iteration 00039    Loss 0.001946   MAE_LR: 0.655051 MAE_LR_avg: 0.782399  SRE: 0.256275 SRE_avg: 0.152139\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}