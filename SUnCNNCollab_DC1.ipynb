{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtLhsZMKazDnNe7mppY7B3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BehnoodRasti/Unmixing-Tutorial/blob/main/SUnCNNCollab_DC1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGZHzt_qoJf6",
        "outputId": "3fc78639-624c-45f4-aad8-cbb82cae55aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'SUnCNN' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/BehnoodRasti/SUnCNN.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import matplotlib.pyplot as plt\n",
        "#%matplotlib inline\n",
        "\n",
        "import os\n",
        "#os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
        "\n",
        "import numpy as np\n",
        "from SUnCNN.models import *\n",
        "\n",
        "import torch\n",
        "import torch.optim\n",
        "\n",
        "from skimage.metrics  import peak_signal_noise_ratio as compare_psnr\n",
        "from skimage.metrics  import mean_squared_error as compare_mse\n",
        "\n",
        "from SUnCNN.utils.denoising_utils import *\n",
        "\n",
        "from skimage._shared import *\n",
        "from skimage.util import *\n",
        "from skimage.metrics.simple_metrics import _as_floats\n",
        "from skimage.metrics.simple_metrics import mean_squared_error\n",
        "\n",
        "\n",
        "from SUnCNN.UtilityMine import *\n",
        "from SUnCNN.utils.sr_utils import tv_loss\n",
        "from numpy import linalg as LA\n",
        "\n",
        "torch.backends.cudnn.enabled = True\n",
        "torch.backends.cudnn.benchmark =True\n",
        "dtype = torch.cuda.FloatTensor\n",
        "\n",
        "PLOT = False\n",
        "import scipy.io\n",
        "#%%\n",
        "fname2  = \"SUnCNN/Data/DC1/Y_clean.mat\"\n",
        "mat2 = scipy.io.loadmat(fname2)\n",
        "img_np_gt = mat2[\"Y_clean\"]\n",
        "img_np_gt = img_np_gt.transpose(2,0,1)\n",
        "[p1, nr1, nc1] = img_np_gt.shape\n",
        "#%%\n",
        "fname3  = \"SUnCNN/Data/DC1/XT.mat\"\n",
        "mat3 = scipy.io.loadmat(fname3)\n",
        "A_true_np = mat3[\"XT\"]\n",
        "\n",
        "#%%\n",
        "fname4  = \"SUnCNN/Data/DC1/EE.mat\"\n",
        "mat4 = scipy.io.loadmat(fname4)\n",
        "EE = mat4[\"EE\"]\n",
        "#%%\n",
        "LibS=EE.shape[1]\n",
        "#%%\n",
        "npar=np.zeros((1,3))\n",
        "npar[0,0]=13.3\n",
        "npar[0,1]=41.4\n",
        "npar[0,2]=130.8\n",
        "#npar[0,3]=367\n",
        "tol1=npar.shape[1]\n",
        "tol2=1\n",
        "save_result=False\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "rmax=5"
      ],
      "metadata": {
        "id": "oNp8Wu7DodSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for fi in tqdm(range(1)):\n",
        "    for fj in tqdm(range(tol2)):\n",
        "            #%%\n",
        "        img_np_gt=np.clip(img_np_gt, 0, 1)\n",
        "        img_noisy_np = add_noise(img_np_gt, 1/npar[0,fi])#11.55 20 dB, 36.7 30 dB, 116.5 40 dB\n",
        "        print(compare_snr(img_np_gt, img_noisy_np))\n",
        "        img_resh=np.reshape(img_noisy_np,(p1,nr1*nc1))\n",
        "        V, SS, U = scipy.linalg.svd(img_resh, full_matrices=False)\n",
        "        PC=np.diag(SS)@U\n",
        "        img_resh_DN=V[:,:rmax]@PC[:rmax,:]\n",
        "        img_noisy_np=np.reshape(np.clip(img_resh_DN, 0, 1),(p1,nr1,nc1))\n",
        "        INPUT = 'noise' # 'meshgrid'\n",
        "        pad = 'reflection'\n",
        "        need_bias=True\n",
        "        OPT_OVER = 'net' # 'net,input'\n",
        "        \n",
        "        # \n",
        "        reg_noise_std = 0.0\n",
        "        LR1 = 0.001\n",
        "        \n",
        "        OPTIMIZER1='adam'# 'RMSprop'#'adam' # 'LBFGS'\n",
        "        show_every = 500\n",
        "        exp_weight=0.99\n",
        "        if fi==0:\n",
        "            num_iter1 = 4000\n",
        "        elif fi==1:\n",
        "            num_iter1 = 8000\n",
        "        elif fi==2:\n",
        "            num_iter1 = 12000\n",
        "        input_depth =img_noisy_np.shape[0]\n",
        "        class CAE_AbEst(nn.Module):\n",
        "            def __init__(self):\n",
        "                super(CAE_AbEst, self).__init__()\n",
        "                self.conv1 = nn.Sequential(\n",
        "                    UnmixArch(\n",
        "                            input_depth, EE.shape[1],\n",
        "                            # num_channels_down = [8, 16, 32, 64, 128], \n",
        "                            # num_channels_up   = [8, 16, 32, 64, 128],\n",
        "                            # num_channels_skip = [4, 4, 4, 4, 4], \n",
        "                            num_channels_down = [ 256],\n",
        "                            num_channels_up =   [ 256],\n",
        "                            num_channels_skip =    [ 4],  \n",
        "                            filter_size_up = 3,filter_size_down = 3,  filter_skip_size=1,\n",
        "                            upsample_mode='bilinear', # downsample_mode='avg',\n",
        "                            need1x1_up=True,\n",
        "                            need_sigmoid=True, need_bias=True, pad=pad, act_fun='LeakyReLU').type(dtype)\n",
        "                )\n",
        "        \n",
        "            def forward(self, x):\n",
        "                x = self.conv1(x)\n",
        "                return x\n",
        "\n",
        "        net1 = CAE_AbEst()\n",
        "        net1.cuda()\n",
        "        print(net1)\n",
        "\n",
        "        # Compute number of parameters\n",
        "        s  = sum([np.prod(list(p11.size())) for p11 in net1.parameters()]); \n",
        "        print ('Number of params: %d' % s)\n",
        "        \n",
        "        # Loss\n",
        "        mse = torch.nn.MSELoss().type(dtype)\n",
        "        img_noisy_torch = np_to_torch(img_noisy_np).type(dtype)\n",
        "        # if fk==0:\n",
        "        net_input1 = get_noise(input_depth, INPUT,\n",
        "          (img_noisy_np.shape[1], img_noisy_np.shape[2])).type(dtype).detach()\n",
        "        net_input1 = img_noisy_torch \n",
        "        E_torch = np_to_torch(EE).type(dtype)\n",
        "        #%%\n",
        "        net_input_saved = net_input1.detach().clone()\n",
        "        noise = net_input1.detach().clone()\n",
        "        out_avg = None\n",
        "        out_HR_avg= None\n",
        "        last_net = None\n",
        "        RMSE_LR_last = 0\n",
        "        loss=np.zeros((num_iter1,1))\n",
        "        AE=np.zeros((num_iter1,1))\n",
        "        i = 0\n",
        "        def closure1():\n",
        "            \n",
        "            global i, RMSE_LR, RMSE_LR_ave, RMSE_HR, out_LR_np, out_avg_np, out_LR\\\n",
        "                , out_avg,out_HR_np, out_HR_avg, out_HR_avg_np, RMSE_LR_last, last_net\\\n",
        "                    , net_input,RMSE_LR_avg,RMSE_HR_avg,RE_HR_avg, RE_HR, Eest,loss,AE\\\n",
        "                       , MAE_LR,MAE_LR_avg,MAE_HR,MAE_HR_avg\n",
        "            \n",
        "            if reg_noise_std > 0:\n",
        "                net_input = net_input_saved + (noise.normal_() * reg_noise_std)\n",
        "            \n",
        "            out_LR = net1(net_input1)\n",
        "            out_HR=torch.mm(E_torch.view(p1,LibS),out_LR.view(LibS,nr1*nc1))\n",
        "            # Smoothing\n",
        "            if out_avg is None:\n",
        "                out_avg = out_LR.detach()\n",
        "                out_HR_avg = out_HR.detach()\n",
        "            else:\n",
        "                out_avg = out_avg * exp_weight + out_LR.detach() * (1 - exp_weight)\n",
        "                out_HR_avg = out_HR_avg * exp_weight + out_HR.detach() * (1 - exp_weight)\n",
        "\n",
        "        #%%\n",
        "            out_HR=out_HR.view((1,p1,nr1,nc1))\n",
        "            total_loss = mse(img_noisy_torch, out_HR)\n",
        "            total_loss.backward()\n",
        "            if True:\n",
        "             out_LR_np = out_LR.detach().cpu().squeeze().numpy()\n",
        "             out_avg_np = out_avg.detach().cpu().squeeze().numpy()\n",
        "             SRE=10*np.log10(LA.norm(A_true_np.astype(np.float32).reshape((EE.shape[1],nr1*nc1)),'fro')/LA.norm((A_true_np.astype(np.float32)- np.clip(out_LR_np, 0, 1)).reshape((EE.shape[1],nr1*nc1)),'fro'))\n",
        "             SRE_avg=10*np.log10(LA.norm(A_true_np.astype(np.float32).reshape((EE.shape[1],nr1*nc1)),'fro')/LA.norm((A_true_np.astype(np.float32)- np.clip(out_avg_np, 0, 1)).reshape((EE.shape[1],nr1*nc1)),'fro'))\n",
        "             MAE_LR= 100*np.mean(abs(A_true_np.astype(np.float32)- np.clip(out_LR_np, 0, 1)))\n",
        "             MAE_LR_avg= 100*np.mean(abs(A_true_np.astype(np.float32)- np.clip(out_avg_np, 0, 1)))\n",
        "             print ('Iteration %05d    Loss %f   MAE_LR: %f MAE_LR_avg: %f  SRE: %f SRE_avg: %f' % (i, total_loss.item(), MAE_LR, MAE_LR_avg, SRE, SRE_avg))\n",
        "\n",
        "            if  PLOT and i % show_every == 0:\n",
        "                out_LR_np = torch_to_np(out_LR)\n",
        "                out_avg_np = torch_to_np(out_avg)\n",
        "        #        plot_image_grid([np.clip(out_np, 0, 1), \n",
        "        #                         np.clip(torch_to_np(out_avg), 0, 1)], factor=figsize, nrow=1)\n",
        "                \n",
        "                # out_LR_np = np.clip(out_LR_np, 0, 1)\n",
        "                # out_avg_np = np.clip(out_avg_np, 0, 1)\n",
        "                \n",
        "                # f, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True, figsize=(10,10))\n",
        "                # ax1.imshow(np.stack((out_LR_np[2,:,:],out_LR_np[1,:,:],out_LR_np[0,:,:]),2))\n",
        "                # ax2.imshow(np.stack((out_avg_np[2,:,:],out_avg_np[1,:,:],out_avg_np[0,:,:]),2))\n",
        "                # ax3.imshow(np.stack((A_true_np[2,:,:],A_true_np[1,:,:],A_true_np[0,:,:]),2))\n",
        "                # plt.show()\n",
        "                plt.plot(out_LR_np.reshape(LibS,nr1*nc1))\n",
        "            loss[i]=total_loss.item() \n",
        "            i += 1\n",
        "        \n",
        "            return total_loss\n",
        "        \n",
        "        p11 = get_params(OPT_OVER, net1, net_input1)\n",
        "        optimize(OPTIMIZER1, p11, closure1, LR1, num_iter1)\n",
        "        if 0:\n",
        "            out_LR_np = out_LR.detach().cpu().squeeze().numpy()\n",
        "            out_avg_np = out_avg.detach().cpu().squeeze().numpy()\n",
        "            MAE_LR_avg= 100*np.mean(abs(A_true_np.astype(np.float32)- np.clip(out_avg_np, 0, 1)))\n",
        "            MAE_LR= 100*np.mean(abs(A_true_np.astype(np.float32)- np.clip(out_LR_np, 0, 1)))\n",
        "            SRE=10*np.log10(LA.norm(A_true_np.astype(np.float32).reshape((EE.shape[1],nr1*nc1)),'fro')/LA.norm((A_true_np.astype(np.float32)- np.clip(out_LR_np, 0, 1)).reshape((EE.shape[1],nr1*nc1)),'fro'))\n",
        "            SRE_avg=10*np.log10(LA.norm(A_true_np.astype(np.float32).reshape((EE.shape[1],nr1*nc1)),'fro')/LA.norm((A_true_np.astype(np.float32)- np.clip(out_avg_np, 0, 1)).reshape((EE.shape[1],nr1*nc1)),'fro'))\n",
        "            print ('Iteration %05d  MAE_LR: %f MAE_LR_avg: %f  SRE: %f SRE_avg: %f ' % (i, MAE_LR, MAE_LR_avg, SRE, SRE_avg))\n",
        "        # if  save_result is True:\n",
        "        #      scipy.io.savemat(\"C:/Users/behnood/Desktop/Sparse Unmixing/Results/Sim2/demo1/10runs/out_avg_np%01d%01d.mat\" % (fi+2,fj+1),\n",
        "        #                     {'out_avg_np%01d%01d' % (fi+2, fj+1):out_avg_np.transpose(1,2,0)})\n",
        "        #      scipy.io.savemat(\"C:/Users/behnood/Desktop/Sparse Unmixing/Results/Sim2/demo1/10runs/out_LR_np%01d%01d.mat\" % (fi+2,fj+1),\n",
        "        #                     {'out_LR_np%01d%01d' % (fi+2, fj+1):out_LR_np.transpose(1,2,0)})\n",
        "#%%\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSpuKXaqrnto",
        "outputId": "0bec89d3-a22c-4f6e-94f0-96fb53875f33"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/1 [00:00<?, ?it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20.172113286242237\n",
            "CAE_AbEst(\n",
            "  (conv1): Sequential(\n",
            "    (0): Sequential(\n",
            "      (1): Concat(\n",
            "        (0): Sequential(\n",
            "          (1): Sequential(\n",
            "            (0): ReflectionPad2d((0, 0, 0, 0))\n",
            "            (1): Conv2d(224, 4, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (3): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (1): Sequential(\n",
            "            (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "            (1): Conv2d(224, 256, kernel_size=(3, 3), stride=(2, 2))\n",
            "          )\n",
            "          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (3): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "          (4): Sequential(\n",
            "            (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "            (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "          )\n",
            "          (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (6): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "          (7): Upsample(scale_factor=2.0, mode=bilinear)\n",
            "        )\n",
            "      )\n",
            "      (2): BatchNorm2d(260, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (3): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(260, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "      )\n",
            "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (6): Sequential(\n",
            "        (0): ReflectionPad2d((0, 0, 0, 0))\n",
            "        (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (8): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (9): Sequential(\n",
            "        (0): ReflectionPad2d((0, 0, 0, 0))\n",
            "        (1): Conv2d(256, 240, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (10): Softmax(dim=None)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Number of params: 1836676\n",
            "Starting optimization with ADAM\n",
            "Iteration 00000    Loss 0.105760   MAE_LR: 0.814688 MAE_LR_avg: 0.814688  SRE: 0.036082 SRE_avg: 0.036082\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 00001    Loss 0.095469   MAE_LR: 0.807976 MAE_LR_avg: 0.814606  SRE: 0.014300 SRE_avg: 0.036613\n",
            "Iteration 00002    Loss 0.085247   MAE_LR: 0.797504 MAE_LR_avg: 0.814369  SRE: 0.038508 SRE_avg: 0.038474\n",
            "Iteration 00003    Loss 0.074961   MAE_LR: 0.785661 MAE_LR_avg: 0.813949  SRE: 0.040503 SRE_avg: 0.041822\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.21s/it]\n"
          ]
        }
      ]
    }
  ]
}